server:
  listen: ":3000"
  read_timeout_ms: 60000
  write_timeout_ms: 60000

auth:
  # Public API key for calling open-next-router
  api_key: "change-me"

providers:
  # Directory containing provider DSL files (*.conf)
  dir: "./config/providers"

keys:
  # Upstream keys file (grouped by provider)
  file: "./keys.yaml"

models:
  # Model routing file (model -> providers)
  file: "./models.yaml"

upstream_proxies:
  # Configure outbound HTTP proxy per provider (optional).
  # Example:
  #   by_provider:
  #     openai: "http://127.0.0.1:7890"
  #     anthropic: "http://127.0.0.1:7891"
  # Supported schemes:
  #   - http:// / https://
  #   - socks5:// / socks5h:// (optional user/pass: socks5://user:pass@host:port)
  by_provider: 

usage_estimation:
  # Estimate token usage when upstream does not return usage (or returns all zeros).
  # This is best-effort and intended for local debugging / rough observability.
  enabled: true
  estimate_when_missing_or_zero: true
  strategy: "heuristic"
  max_request_bytes: 1048576
  max_response_bytes: 1048576
  max_stream_collect_bytes: 262144
  apis: ["chat.completions", "responses", "claude.messages", "embeddings", "gemini.generateContent", "gemini.streamGenerateContent"]

traffic_dump:
  # Enable traffic dump logs to files (request/response capture, best-effort masking).
  enabled: false
  dir: "./dumps"
  file_path: "{{.request_id}}.log"
  max_bytes: 1048576
  mask_secrets: true

logging:
  level: "info"
  access_log: true
